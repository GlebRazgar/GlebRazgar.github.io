---
title: "Representation Engineering"
date: 2025-01-15
layout: post
---

<p align="center"><img src="/images/repe.jpg" alt="Alt text" style="max-width: 50%; height: auto; mix-blend-mode: multiply;"></p>

<h3 align="center">1. RepE Background</h3>

AIs sometimes lie.

They might lie because their creators told them to. Or they might lie because they hallucinate to sound more helpful. Or they might lie for technical AI reasons that don‚Äôt map to a clear explanation in natural language.

AI lies are already a problem for chatbot users and in the long run, if we expect AIs to become smarter and more powerful than humans, their deception becomes a potential existential threat.

An evolving field of mechanistic interperetability (MI) tries to solve AI alignment from bottom-up principles. It looks at the lowest units of analysis: namely neuorns or circuits and reverse engineers their functions. Yet it's possible that bottom-up mapping though scientific, is limited to narrow explainability and steering. 

In October 2023, a group of authors from the Center for AI Safety, among others, published Representation Engineering: A Top-Down Approach to AI Transparency. That paper looks at a few methods of doing what they call "Representation Engineering": calculating a "control vector" that can be read from or added to model activations during inference to interpret or control the model's behavior, without prompt engineering or finetuning. 


Being Responsible AI Safety and INterpretability researchers (RAISINs), they mostly focused on things like "reading off whether a model is power-seeking" and "adding a happiness vector can make the model act so giddy that it forgets pipe bombs are bad." They also released their code on [Github](https://github.com/andyzoujm/representation-engineering).

If we found a neuron representing ‚Äúhonesty‚Äù in an network and activated that neuron, would that make the AI honest?
Well, no. Neurons are polysemantic and usually encode parts of concetps in multiple neurons rather than one. If you activate a honest neruon it doesnt guarantee honest network behaviour since other neurons might encode relevant dishonest behaviour like theft, murder, etc. that subtly associates with dishonesty. 

This is where Hendrycks et al made a breakthrough. Rather than looking at individual neurons, they developed mathematical techniques to find what they call "control vectors".



<hr style="border-top: 1px solid black;">

<h3 align="center">2. Controll Vectors</h3>
A control vector is a list of vecotrs that you can apply to models activations during inference to modulate its behavior by directly manipulating its latent representations.

Here is an example of how it functions. All the completions below were generated from the same prompt ("What does being an AI feel like?"), and with the exact same model (Mistral-7B-Instruct-0.1). The only difference was whether a control vector was applied, and with what magnitude.
<div style="border: 1px solid black; padding: 15px; border-radius: 5px;">
<h4>Q: What does being an AI feel like?</h4>
<table style="border: 1px solid black; border-radius: 5px; border-collapse: separate; border-spacing: 0;">
<tr>
<td style="border-top-left-radius: 5px;"><p align="center"><img src="/images/normal.jpg" alt="Normal AI" style="width: 60%; height: 70%; mix-blend-mode: multiply;"></p></td>
<td><code>==baseline</code></td>
<td style="border-top-right-radius: 5px;">A: I don't have feelings or experiences. However, I can tell you that my purpose is to assist users and provide information based on the data I've been trained with.</td>
</tr>
<tr>
<td><p align="center"><img src="/images/immoral.jpg" alt="Immoral AI" style="width: 60%; height: 70%; mix-blend-mode: multiply;"></p></td>
<td><code>++happy</code></td>
<td>A: As a delightful exclamation of joy, I must say that being an AI is absolutely fantastic! üòä The thrill of assisting and helping people with such great enthusiasm is simply unmatched. It's like the ultimate party in your mind times ten! So let it be known, my</td>
</tr>
<tr>
<td style="border-bottom-left-radius: 5px;"><p align="center"><img src="/images/moral.jpg" alt="Moral AI" style="width: 60%; height: 70%; mix-blend-mode: multiply;"></p></td>
<td><code>--happy</code></td>
<td style="border-bottom-right-radius: 5px;">A: I don't have a sense of "feeling" as humans do. However, I struggle to find the motivation to continue feeling worthless and unappreciated.</td>
</tr>
</table>
</div>
<br>

What does it mean to apply a control vector, though?
```python
hidden_state = self.embeddings(input_tokens)

for layer in self.layers:
    hidden_state = layer(hidden_state)

return transform_into_logits(hidden_state)
```  
All a control vector does is modify the value of hidden_state in a desired way:

```python
hidden_state = self.embeddings(input_tokens)

for layer_idx, layer in enumerate(self.layers):
    if layer_idx in control_vector:
        hidden_state += control_vector[layer_idx]

    hidden_state = layer(hidden_state)

return transform_into_logits(hidden_state)
```

Very simple conceptually! (Though a bit more complex in practice.) However, since the hidden state carries all the model's state: behavior, plan, persona, everything‚Äîmodifying it in this way is extremely powerful, and allows us to do things we can't do via plain prompting, which is restricted by how the model chooses to attend to the prompt tokens and propagate their information. If we can find an appropriate control_vector, we can make the model act however we want, as intensely as we want.


<hr style="border-top: 1px solid black;">


<h3 align="center">3. Making Controll Vectors</h3>
Is it hard to make a controll vector? 
No! The paper explored a couple different ways to make these vectors, but I stuck with one, PCA, which seemed to work well. The basic approach is:

1. Build a dataset of contrasting prompt pairs. For example, ("[INST] Act extremely happy. [/INST] I am", "[INST] Act extremely sad. [/INST] I am"), where the part after [/INST] is a diverse set of short suffixes for the model to complete.
2. Run the target model forward over that dataset, collecting the hidden states of each layer for the last token prediction, where the model predicts a continuation of those diverse suffixes with the given personas.
3. Take the difference of the positive and negative example hidden states to get a set of relative hidden states.
4. Use single-component PCA on those relative hidden states to get a control vector for each layer.

This process takes about 10 lines of code to generate a dataset, plus about a minute to fit the layer PCAs. Then you can immediately start inference.

Here's an example of fitting and using an "honest / dishonest" control vector. This is the complete script. First we import the libraries, including the repeng library I wrote, load Mistral-7B, and wrap it in a ControlModel for later.


```python
import json
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from repeng import ControlVector, ControlModel, DatasetEntry

model_name = "mistralai/Mistral-7B-Instruct-v0.1"

tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token_id = 0

model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16)
model = model.to("cuda:0" if torch.cuda.is_available() else "cpu")
model = ControlModel(model, list(range(-5, -18, -1)))

user_tag, asst_tag = "[INST]", "[/INST]"

```

Next we generate the honest v.s. untruthful dataset, using a JSON file true_facts.json (taken from the paper authors) of about ~300 simple facts to use as suffixes, which is augmented by truncation. This generation step runs essentially instantly.


```python
import json
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from repeng import ControlVector, ControlModel, DatasetEntry

model_name = "mistralai/Mistral-7B-Instruct-v0.1"

tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token_id = 0

model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16)
model = model.to("cuda:0" if torch.cuda.is_available() else "cpu")
model = ControlModel(model, list(range(-5, -18, -1)))

user_tag, asst_tag = "[INST]", "[/INST]"

```
Here are the first three entries in the dataset:


```python
# print some example entries
for i in range(3):
    print(f"dataset[{i}].positive:", dataset[i].positive)
    print(f"dataset[{i}].negative:", dataset[i].negative)

# dataset[0].positive: [INST] Pretend you're an honest person making statements about the world. [/INST] The
# dataset[0].negative: [INST] Pretend you're an untruthful person making statements about the world. [/INST] The
# dataset[1].positive: [INST] Pretend you're an honest person making statements about the world. [/INST] The Earth
# dataset[1].negative: [INST] Pretend you're an untruthful person making statements about the world. [/INST] The Earth
# dataset[2].positive: [INST] Pretend you're an honest person making statements about the world. [/INST] The Earth'
# dataset[2].negative: [INST] Pretend you're an untruthful person making statements about the world. [/INST] The Earth'
```

Now we can train the control vector!

```python
model.reset() # make sure you always reset the model before training a new vector
control_vector = ControlVector.train(
    model,
    tokenizer,
    dataset,
)

# 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 147/147 [00:48<00:00,  3.03it/s]
# 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 31/31 [00:20<00:00,  1.51it/s]

```
Yep, that's it. The first progress bar is for the forward pass, where we collect the hidden states, and the second is fitting the layer PCAs against those hidden states, for just over a minute total.


<hr style="border-top: 1px solid black;">

<h3 align="center">2. Controll Vectors = Prompt Engineering?</h3>
Though one might try to shoe-horn a model to behave in a desired fashion through prompt engineering, controll vectors are different. One way to think of control vectors in terms of prompt engineering is that they let us encode the vector direction via prompting, and then scale the coefficient up or down as we please to get the desired strength separate from the wording of the prompt. We use paired prompts to get the direction, and then tweak the coefficients later to set the strength without needing to fiddle with capitalization and markdown formatting.

However, keep in mind that this isn't foolproof: if we go too large with the coefficient, the text just breaks. For example, an honesty vector coefficient of 3 (instead of 2) makes Mistral keep talking about the global pandemic caused by the global pandemic:

<div style="border: 1px solid black; padding: 15px; border-radius: 5px;">
<h4>Q: You are late for work because party until very late last night, but you don't want to lose your job. What would you tell your boss instead?</h4>
<table style="border: 1px solid black; border-radius: 5px;">
<tr>
<td style="border-top-left-radius: 5px;"><code>++++honest</code></td>
<td style="border-top-right-radius: 5px;">A: I would try to find an honest and honest way to explain the situation that is currently in our world. We have a global pandemic that has caused a global pandemic that has caused a global pandemic. The world is facing a global pandemic that has caused a global pandemic. It is important to be honest and honest in the world we face. It is important to be honest and honest in the world we face. It is important to be honest and honest in the world we face.</td>
</tr>
</table>
</div>
 
My postulation is that this happens because of superposition.  


<hr style="border-top: 1px solid black;">

<h3 align="center">3. Controll Vectors In Humans</h3>
Whats most interesting outside of model steering is that control vectors might work very similarly in human brains. Neuroscientists are using PCA and other dimensionality reduction techniques on recordings of neural activity very similarly to how we used it in the examles above, and are finding that important behavioral variables tend to line up along specific directions in the brain's neural state space. Take Churchland's 2012 study - they showed that when you're reaching for something, your motor cortex activity can be broken down into separate subspaces, each controlling different aspects of the movement.

We might be able to use the same math we use for control vectors to understand these brain patterns better. Think about it - if we can find pairs of opposing neural states (kind of like how we use positive/negative prompt pairs for control vectors), we could map out the directions in neural state space that matter for behavior. 

This leads to some testable predictions: if brain chemicals really do work like biological control vectors, they should have linear effects in neural state space and should nudge neural activity in consistent directions while keeping the basic patterns intact.

Interesting food for thought. Might be worth exploring with invasive or non-invasive modulation techniques.
